Absolutely, Apurva. Let‚Äôs bring your SRO-53 ‚Üí SSRS-305 pair to full QA excellence using your Automation Test Engineer persona.


---

üü© System Requirement: SEGV-SRO-53

> ‚ÄúThe system shall provide a set of default hanging protocols.‚Äù



‚úÖ Objective:

Ensure the system provides fallback display logic (via default HPs) when no learned/user-defined HP is matched.


---

üü® Sub-System Requirement: SEGV-SSRS-305

> ‚ÄúThe system shall display a 1x1 viewport grid on the primary monitor containing the first series from the primary study when no other layout definition is present.‚Äù



‚úÖ Objective:

If no HP is matched, system falls back to showing the first series from the primary study in a 1x1 layout on the primary monitor.


---

‚úÖ All Descriptive Test Scenarios for SRO-53 & SSRS-305

(Written using ‚ÄúVerify‚Ä¶‚Äù format, with Positive, Negative, and Edge scenarios‚Äîfully grounded in UI, backend, DICOM, and workflow behavior)


---

‚úÖ Positive Test Scenarios

1. Verify that the system applies the default 1x1 layout when no learned or user-defined hanging protocol is matched for a study


2. Verify that the first available series from the primary study is displayed in the 1x1 viewport when no layout is defined


3. Verify that the fallback 1x1 layout appears specifically on the primary monitor in single-monitor configurations


4. Verify that the system applies the fallback default layout consistently across modalities such as CT, MR, and US


5. Verify that the system applies the default layout even when the study contains multiple series without a matched HP


6. Verify that the default layout configuration is stored and retrieved correctly from system settings or configuration service


7. Verify that default HP is used when a new user with no preferences logs in and opens a study for the first time




---

‚ùå Negative Test Scenarios

8. Verify that the system displays a message or fallback content when the primary study contains no valid image series


9. Verify that the system does not crash when the default HP layout file is corrupted or malformed


10. Verify that an empty viewport is shown if the first series is not retrievable due to missing SOPInstanceUID or broken DICOM tags


11. Verify that the fallback layout still loads if the monitor configuration metadata is missing or incorrect


12. Verify that the viewer logs a warning or error if the default HP references a missing display group


13. Verify that no other protocols (manual, learned) override the fallback 1x1 layout once it's applied




---

‚ö†Ô∏è Edge Test Scenarios

14. Verify that when multiple series have the same Series Number, the viewer deterministically selects the first by acquisition time or SOPInstanceUID


15. Verify that in dual-monitor setups, the fallback 1x1 layout still renders on the primary monitor, not arbitrarily


16. Verify that the fallback 1x1 layout appears even when the study has partial metadata or non-standard compression (e.g., JPEG2000)


17. Verify that fallback 1x1 layout displays correctly on high-resolution or ultrawide monitors without stretching or misalignment


18. Verify that fallback applies instantly if the SRP engine times out or is disabled in configuration


19. Verify that fallback layout is not applied if a user-selected protocol exists even if SRP fails


20. Verify correct behavior when the system switches between online and offline mode (SRP service toggle)


21. Verify that the viewer reverts to fallback layout when the previously saved Learn Case was deleted before viewer launch




---

‚úÖ Relevant Acceptance Criteria (AC) Mapped to Test Scenarios

Acceptance Criteria	Covered By Test Cases

AC1: Default HP must apply when no other protocol is available	1, 2, 6, 7
AC2: Fallback layout must be 1x1	1, 3, 4, 5, 14
AC3: First series of primary study must be shown	2, 8, 10, 14
AC4: Must not crash or degrade in corrupted/missing config	9, 10, 11, 12
AC5: Must apply on primary monitor	3, 15
AC6: Must support edge devices and dual-screen scenarios	15, 17
AC7: System must distinguish between fallback vs matched protocol state	13, 19, 21



---

Would you like these transformed into:

Gherkin-style feature file

Manual test case table (with steps + expected)

Playwright automation code scaffold


Or should we move to the next SRO/SSRS pair?
Absolutely, Apurva! Here‚Äôs a clean separation of test cases:


---

üü© SRO-53: ‚ÄúThe system shall provide a set of default hanging protocols.‚Äù

‚úÖ End-to-End Test Scenarios for SRO-53

These validate the system-level behavior of fallback hanging protocols being provided and applied when no user-learned or predefined layout is available.

üî∑ Positive

1. Verify that the system applies the default 1x1 layout when no learned or user-defined hanging protocol is matched for a study


2. Verify that the default layout configuration is stored and retrieved correctly from system settings or configuration service


3. Verify that default HP is used when a new user with no preferences logs in and opens a study for the first time


4. Verify that the system applies the default layout consistently across different modalities such as CT, MR, and US



üî∂ Negative

5. Verify that the viewer does not crash when the default HP layout file is corrupted or malformed


6. Verify that the viewer logs a warning or error if the default HP references a missing display group


7. Verify that fallback layout is not applied if a user-selected protocol exists even if SRP fails



‚ö†Ô∏è Edge

8. Verify that fallback applies instantly if the SRP engine times out or is disabled in configuration


9. Verify correct behavior when the system switches between online and offline mode (SRP service toggle)


10. Verify that the viewer reverts to fallback layout when the previously saved Learn Case was deleted before viewer launch




---

üü® SSRS-305: ‚ÄúThe system shall display a 1x1 viewport grid on the primary monitor containing the first series from the primary study when no other layout definition is present.‚Äù

‚úÖ Functional Test Scenarios for SSRS-305

These focus on the actual fallback layout rendering behavior on the viewer side.

üî∑ Positive

1. Verify that the first available series from the primary study is displayed in the 1x1 viewport when no layout is defined


2. Verify that the 1x1 layout is displayed on the primary monitor in a single-monitor setup


3. Verify that the fallback layout is modality-agnostic and applies uniformly across supported modalities


4. Verify that the fallback 1x1 layout does not interfere with subsequent manual layout overrides by the user



üî∂ Negative

5. Verify that the system displays a message or fallback content when the primary study contains no valid image series


6. Verify that an empty viewport is shown if the first series is not retrievable due to missing SOPInstanceUID or broken DICOM tags


7. Verify that the fallback layout still loads if the monitor configuration metadata is missing or incorrect


8. Verify that no other protocols (manual, learned) override the fallback 1x1 layout once it is applied



‚ö†Ô∏è Edge

9. Verify that when multiple series have the same Series Number, the viewer deterministically selects the first by acquisition time or SOPInstanceUID


10. Verify that in dual-monitor setups, the fallback 1x1 layout still renders on the primary monitor, not arbitrarily


11. Verify that the fallback 1x1 layout displays correctly on high-resolution or ultrawide monitors without distortion


12. Verify that fallback 1x1 layout handles DICOM studies with non-standard compression (e.g., JPEG2000)




---

Would you like me to generate these in a structured table or export them into a specific format (Excel test suite, Gherkin, JSON)?
Or are we moving on to the next SRO/SSRS pair?

‚Äê---------------------
Thanks, Apurva. Let's continue with SRO-69 and its related SSRS-282, 306, and 307. I‚Äôve separated the test cases based on SRO and SSRS, and each scenario starts with "Verify" as you prefer.


---

‚úÖ SRO-69

Requirement: "The system shall allow user to save the current hanging state as a Learn Case."

üîπ End-to-End Test Scenarios for SRO-69

1. Verify that the system allows the user to trigger Learn Case saving from the viewer UI.


2. Verify that the system persists all applied hanging parameters in the saved Learn Case.


3. Verify that saving a Learn Case does not affect the ongoing study viewing session.


4. Verify that the system shows a success message upon successful Learn Case saving.


5. Verify that Learn Case saving fails gracefully if the backend service is down.


6. Verify that Learn Case saving is disabled for unsupported modalities.


7. Verify that Learn Case overwrites existing case if one already exists for the same user and context (if designed so).


8. Verify that Learn Case is saved with proper metadata: timestamp, user ID, study UID, etc.


9. Verify that saving is restricted when the user has read-only access.


10. Verify that system logs capture Learn Case save event with proper audit information.




---

‚úÖ SSRS-282

Requirement: "Front end application shall provide a Learn button to save the current hanging protocol."

üîπ Test Scenarios for SSRS-282

1. Verify that the Learn button is visible when a hanging protocol is applied.


2. Verify that clicking the Learn button initiates saving of current hanging parameters.


3. Verify that Learn button is disabled if there is no active hanging protocol.


4. Verify tooltip/help text is shown when hovering over the Learn button.


5. Verify button responsiveness across different resolutions and screen modes.


6. Verify that clicking Learn multiple times rapidly does not trigger duplicate saves.


7. Verify Learn button behavior for different roles: Radiologist vs Technician.


8. Verify UI feedback (e.g., spinner or success icon) after clicking Learn.


9. Verify that Learn button is not rendered on mobile mode or unsupported browsers.




---

‚úÖ SSRS-306

Requirement: "The system shall display a message indicating the SRP Learn Case could not be saved."

üîπ Test Scenarios for SSRS-306

1. Verify that an error message is displayed when Learn Case save fails due to server error.


2. Verify that message shows proper context like: ‚ÄúSave failed. Try again later.‚Äù


3. Verify that the error message disappears after a timeout or user dismissal.


4. Verify that no message is shown if save succeeds.


5. Verify that repeated save failures do not stack multiple messages.


6. Verify that localized error messages are shown in selected user language.


7. Verify that error message appears when the internet connection is lost during saving.


8. Verify that proper status code from API maps to meaningful user messages (e.g., 400, 500).




---

‚úÖ SSRS-307

Requirement: "The system shall wait for a response from the HP Service up to a configurable time threshold before applying a default hanging protocol."

üîπ Test Scenarios for SSRS-307

1. Verify that system waits for a Learn Case match response from HP service before applying HP.


2. Verify that default HP is applied when no response is received within the configured timeout.


3. Verify that the timeout is configurable from admin or config panel.


4. Verify system logs the timeout event for HP fallback usage.


5. Verify HP matching logic is skipped if default timeout is reached.


6. Verify that increasing the timeout value delays fallback to default HP.


7. Verify behavior when HP service sends partial or malformed response.


8. Verify fallback mechanism does not apply if timeout is set to 0.


9. Verify correct HP is applied if response is received just before timeout.




---

Let me know if you‚Äôd like the same format for another SRO next.

---------------72----------------
Absolutely, Apurva. Here's the full breakdown for:


---

üü© SRO-72

Requirement: "The system shall allow the user to select an alternate hanging protocol from a list of available hanging protocols."

> üîÅ This requirement focuses on enabling the user-driven selection of an HP (instead of automatic SRP selection) ‚Äî directly involving user interaction with UI (Learn dropdown) and confirming HP application.




---

‚úÖ End-to-End Test Scenarios for SRO-72

1. Verify that the system displays a list of available hanging protocols for user selection when the viewer is launched.


2. Verify that the user can open the list of hanging protocols through the UI dropdown.


3. Verify that the viewer updates its layout when a user selects a hanging protocol from the list.


4. Verify that only the selected hanging protocol is applied and no auto-selection overrides it.


5. Verify that the viewer maintains state consistency after switching from an auto-applied HP to a manually selected one.


6. Verify that the list of alternate HPs is generated based on Learn Cases relevant to the current study and context.


7. Verify that the applied protocol reflects in the audit trail with timestamp and user ID.


8. Verify that protocol selection remains stable during comparison addition/removal.


9. Verify that protocol switching is blocked or warned if done mid-way through a reading session (depending on business rules).


10. Verify that when no hanging protocols are available, the UI handles the empty state gracefully.




---

üü® SSRS-301

Requirement: ‚ÄúThe front-end application shall provide a drop-down menu from the HP Learn button with each menu item labeled with HP name and context matching score.‚Äù


---

üîπ Test Cases for SSRS-301

1. Verify that clicking the HP Learn button displays a dropdown with alternate hanging protocols.


2. Verify that each entry in the dropdown shows the hanging protocol name.


3. Verify that each hanging protocol is labeled with its context matching score.


4. Verify that the dropdown list is sorted by descending match score.


5. Verify that when no Learn Cases are available, the dropdown is either empty or hidden.


6. Verify tooltip, hover state, or extended info is shown for long-named protocols.


7. Verify that dropdown rendering adapts properly on small or ultra-wide screen sizes.


8. Verify that the dropdown can be navigated via keyboard (up/down/enter).


9. Verify that the dropdown closes when clicking outside the viewer frame or menu.


10. Verify that the matching score shown is accurate and comes from the SRP engine logic.




---

üü® SSRS-302

Requirement: ‚ÄúUpon user selection of a specific menu item from the HP Learn button drop-down, the front-end application shall apply the selected Hanging Protocol.‚Äù


---

üîπ Test Cases for SSRS-302

1. Verify that selecting a hanging protocol from the dropdown applies it immediately in the viewer.


2. Verify that the selected protocol's layout, viewport mapping, and rendering parameters are reflected in the viewer state.


3. Verify that applying a new HP replaces the previously applied HP completely.


4. Verify that a loading spinner or message is shown while the selected HP is being applied.


5. Verify that an error message is shown when applying a corrupted or unavailable HP.


6. Verify that user selection of HP is recorded in logs or audit trail with user ID and timestamp.


7. Verify that if the user re-selects the same HP, no redundant re-application occurs.


8. Verify that the system can gracefully apply HP even if some referenced series are missing.


9. Verify that each component of the selected HP (layout, sync tools, cross-ref) is respected in the UI after application.


10. Verify fallback behavior when HP application partially fails (e.g., missing one series in layout).




---

‚úÖ Acceptance Criteria Mapping (AC)

AC Description	Covered In Test Cases

User should be able to see all matching alternate HPs	SRO-72 TC 1, SSRS-301 TC 1, 2, 4
User should see match score for context relevance	SSRS-301 TC 3, 10
User must be able to select and apply a hanging protocol	SRO-72 TC 3, SSRS-302 TC 1, 2
System must apply selected HP and update viewer layout	SSRS-302 TC 2, 3, 9
Audit trail must reflect user protocol selection	SRO-72 TC 7, SSRS-302 TC 6
Viewer must handle errors and unavailable/malformed HPs	SSRS-302 TC 5, 10
Dropdown should be fully functional (keyboard, responsive, accessible)	SSRS-301 TC 7, 8, 9



---

Would you like the same level of detail for SRO-73 or 75 next, or convert these into a tabular format or Gherkin?

-------------------73----------------

Certainly, Apurva! Here's the detailed breakdown for:


---

üü© SRO-73

Requirement:
‚ÄúThe system shall support the selection of comparison studies as part of hanging protocol application.‚Äù


---

‚úÖ End-to-End Test Scenarios for SRO-73

1. Verify that the system allows the user to select one or more comparison studies during hanging protocol application.


2. Verify that the selected comparison studies are included in the context used for hanging protocol matching.


3. Verify that the viewer correctly displays both current and selected comparison studies after HP is applied.


4. Verify that Learn Case selection and scoring include the selected comparison studies in context matching.


5. Verify that removing a comparison study re-evaluates the hanging protocol if supported by the system design.


6. Verify that HPs with embedded comparison logic behave correctly when comparisons are added manually.


7. Verify that system audit logs include which comparison studies were selected when applying the HP.


8. Verify fallback behavior when comparison studies are not available or cannot be loaded.




---

üü® SSRS-303

Requirement:
‚ÄúThe front-end application shall include comparison studies as part of the study context in the hanging protocol.‚Äù


---

üîπ Test Cases for SSRS-303

‚úÖ Positive Scenarios

1. Verify that the viewer includes selected comparison studies as part of the hanging protocol matching context.


2. Verify that comparison studies are displayed in the designated comparison viewports as per the hanging protocol definition.


3. Verify that the HP engine re-evaluates and ranks Learn Cases based on the comparison study‚Äôs context match.


4. Verify that when two Learn Cases differ only by comparison study presence, the better match is selected.


5. Verify that multiple comparison studies can be selected and included in the Learn Case context.



‚ùå Negative Scenarios

6. Verify that an error is displayed if a selected comparison study is corrupt or fails to load.


7. Verify that the system skips unusable comparison studies during protocol application.


8. Verify that mismatched modality comparison studies do not break the layout or matching logic.



‚ö†Ô∏è Edge Scenarios

9. Verify that if comparison studies are added after HP is applied, the viewer doesn‚Äôt reapply HP unless triggered.


10. Verify that selecting a comparison study with only non-diagnostic images does not interfere with HP layout.


11. Verify that the HP service still functions when a mix of DICOM-compliant and non-compliant comparison studies is used.


12. Verify that comparison study metadata (e.g., StudyDate, Modality, StudyUID) is passed correctly in the API call for HP selection.




---

‚úÖ Acceptance Criteria Mapping

Acceptance Criteria	Covered Test Cases

HP context should include selected comparison studies	SRO-73 TC 2, SSRS-303 TC 1, 3, 12
Viewer should render comparison studies alongside current study	SRO-73 TC 3, SSRS-303 TC 2
Matching should prioritize Learn Cases with better comparison context match	SRO-73 TC 4, SSRS-303 TC 4
Comparison studies should not cause errors or break layout	SRO-73 TC 8, SSRS-303 TC 6, 7, 8
System should handle multi-comparison gracefully	SSRS-303 TC 5, 9, 10, 11
Proper logging and metadata passing required	SRO-73 TC 7, SSRS-303 TC 12



---

Let me know if you'd like this batch exported in test case table format, or if you‚Äôre ready to proceed to SRO-75 or another next pair.


Absolutely, Apurva. You‚Äôve got it ‚Äî I‚Äôll now dissect SSRS-289 down to the finest implementation detail, just like a 10+ year automation test engineer would for a radiology imaging system like GenesisViewer.


---

üü® SSRS-289 (Detailed Breakdown)

Requirement: "The front-end application shall apply the display/rendering parameters as per the hanging protocol for each viewport."
Applies To: GenesisViewer (HP Client)
Parent SRO: SRO-74


---

üîç Key Functional Components to Cover in Testing:

Rendering Parameter	DICOM Tag(s) / Source	Notes

WW/WL	(0028,1050) / (0028,1051)	Must apply per viewport per series
Zoom & Pan	Internal viewer state (usually float/scalar values)	May vary by modality or protocol
Orientation	(0020,0037) ‚Äì Image Orientation Patient	Critical for anatomical alignment
Shutters	(0018,1622)‚Äì(0018,1624) or Presentation State	Optional ‚Äì should not break viewer
VOI LUT / Presentation State	(0028,3010) / (0028,3110) etc.	DICOM-defined grayscale transforms
Per-viewport mapping	Series UID or display group mapping	Must not cross-apply settings



---

‚úÖ Test Cases ‚Äì Organized by Parameter & Scenario

Each starts with "Verify" and is grouped by what rendering logic is being tested.


---

üß™ A. WW/WL (Window Width / Window Level)

1. Verify that the WW/WL values defined in the hanging protocol are applied to each viewport upon layout initialization.


2. Verify that different WW/WL values are applied per viewport when different series are loaded.


3. Verify that default WW/WL presets are used when HP does not define values.


4. Verify that WW/WL values are overridden only when the user explicitly changes them.


5. Verify that reapplying the same HP resets WW/WL to original values.


6. Verify that WW/WL values persist through study reload when HP is reapplied.




---

üß™ B. Zoom & Pan

7. Verify that zoom values defined in the HP are correctly applied to each viewport.


8. Verify that pan (x/y offset) is correctly restored per viewport from the HP.


9. Verify that zoom/pan settings are not applied globally unless the HP defines global sync.


10. Verify correct scaling/centering behavior on high-DPI displays or ultrawide screens.


11. Verify zoom behavior is modality-aware (e.g., disable for ultrasound if defined so).


12. Verify fallback to default zoom when HP zoom parameters are invalid or missing.




---

üß™ C. Orientation

13. Verify that image orientation (e.g., HFS, HFP) is applied as defined in the HP per viewport.


14. Verify that each viewport respects its assigned orientation regardless of other viewports.


15. Verify that flipping or rotating images based on orientation settings reflects in overlay.


16. Verify viewer gracefully handles missing or malformed orientation tags.


17. Verify that user override does not persist after HP is re-applied.




---

üß™ D. Shutters & VOI LUTs / Presentation State

18. Verify that shutters are rendered in viewports when specified in the HP.


19. Verify that VOI LUTs are applied per series if provided in the HP.


20. Verify that missing or malformed LUTs result in safe fallback (grayscale default).


21. Verify that overlapping shutters from DICOM Presentation State are ignored if HP disables them.


22. Verify correct rendering when Presentation State is present but HP defines custom WW/WL.




---

üß™ E. Per-Viewport Isolation

23. Verify that rendering settings in one viewport do not affect another.


24. Verify that switching series in one viewport maintains its unique render settings.


25. Verify that layout reset applies only the HP settings, not cached user overrides.




---

üß™ F. General/Meta Tests

26. Verify that rendering parameters are applied in the correct order: layout ‚Üí mapping ‚Üí rendering.


27. Verify that hanging protocol reapplication completely resets the viewport state.


28. Verify that audit logs reflect which rendering parameters were applied per viewport.


29. Verify that mismatched series types (e.g., one MPR and one 2D axial) still receive correct rendering.


30. Verify consistent rendering behavior when switching between Learn Case and default HP.




---

‚ö†Ô∏è Edge Case Test Scenarios

31. Verify that rendering applies correctly to multi-frame DICOM series.


32. Verify rendering in stacked view mode where the same series spans multiple viewports.


33. Verify correct behavior when images are compressed (e.g., JPEG2000, RLE).


34. Verify behavior when SOPInstanceUID referenced in HP is not present in current study.


35. Verify that rendering logic gracefully handles series of vastly different dimensions or matrix sizes.




---

‚úÖ Acceptance Criteria (Mapped)

Acceptance Criteria	Covered Test Cases

Rendering parameters from HP must be applied to all viewports	1‚Äì6, 7‚Äì12, 13‚Äì17
Each viewport must have isolated rendering behavior	23‚Äì25
HP logic must handle missing/invalid rendering data gracefully	3, 6, 12, 16, 20, 34
VOI LUTs, shutters, and orientation must not cause viewer instability	18‚Äì22, 29, 33
Auditability and logging must trace rendering application	28
Rendering order and layout interaction must be correct	26, 27
Compatibility with compressed, multi-frame, and large image data	31‚Äì35



---

Would you like these converted into:

Structured test case document (Excel/test suite)?

Gherkin for automation?

Or shall we proceed with SSRS-290 in this same deep format?


Let‚Äôs keep the momentum going.

---------------------290---------------
Here‚Äôs the minute-level breakdown and full set of ‚ÄúVerify‚Ä¶‚Äù test scenarios for SSRS-290, organized by each global function. I‚Äôve also included the relevant acceptance criteria mapping at the end.


---

üü® SSRS-290

Requirement:
‚ÄúThe front-end application shall enable/disable the global application functions like Sync, 3D Cursor, Cross Ref, Stack View for each viewport based on the hanging protocol.‚Äù

Parent SRO: SRO-74
Impact: HP Client (GenesisViewer front-end)


---

üîç Key Functions & Implementation Notes

Function	Description	UI Element / State Source

Sync	Synchronizes scrolling/zooming/panning across viewports	Toggle button; internal sync flag
3D Cursor	Shows/hides 3D cursor crosshair and ROI tracking across series	Cursor icon; 3D cursor state
Cross Ref	Cross-reference lines between viewports to correlate anatomy	Overlay lines; cross-ref flag
Stack View	Enables cine/stack playback mode for multi-frame series	Stack/play button; stack mode state



---

‚úÖ Detailed Test Scenarios

A. Sync

1. Verify that Sync is enabled in all viewports when the hanging protocol defines synchronization


2. Verify that all synchronized viewports scroll together when the user scrolls one viewport


3. Verify that zooming in one viewport zooms all synced viewports by the same factor


4. Verify that pan operations in one viewport pan all synced viewports equally


5. Verify that Sync toggle in the toolbar reflects the HP setting (on/off) upon HP application


6. Verify that disabling Sync in the HP results in independent scrolling/zoom behavior per viewport



Negative & Edge
7. Verify that Sync does not activate if there is only one viewport
8. Verify that Sync toggling while cine play is active does not crash the viewer
9. Verify that partial sync (e.g., only zoom synced) is applied if the HP specifies sub-feature granularity
10. Verify that sync state persists through layout re-applications but resets when a different HP is chosen


---

B. 3D Cursor

11. Verify that the 3D Cursor is visible in viewports when enabled by the hanging protocol


12. Verify that moving the cursor in one viewport updates the cursor position in all linked 3D viewports


13. Verify that the 3D Cursor toggle reflects the HP definition immediately after HP is applied


14. Verify that disabling the 3D Cursor in the HP hides the cursor overlay in all viewports



Negative & Edge
15. Verify that 3D Cursor operations do not throw exceptions when applied to single-slice series
16. Verify that the cursor update logic handles missing slice positions gracefully (e.g., gaps in Z-axis)
17. Verify that enabling 3D Cursor on a non-volume series (e.g., CR) has no effect and shows no errors
18. Verify that rapid toggling of 3D Cursor does not lead to state desynchronization between viewports


---

C. Cross Ref

19. Verify that Cross-Reference lines appear across viewports when enabled by the hanging protocol


20. Verify that moving the reference line in one viewport updates the corresponding lines in all others


21. Verify that the Cross Ref toggle state matches the HP definition upon HP load


22. Verify that disabling Cross Ref removes all overlay lines immediately



Negative & Edge
23. Verify that Cross Ref does not apply on modalities that lack spatial correlation (e.g., US)
24. Verify that malformed line coordinates in the HP do not crash the viewer but fall back silently
25. Verify that Cross Ref functionality survives viewport reordering or layout changes mid-session


---

D. Stack View (Cine Mode)

26. Verify that Stack View (cine playback controls) is enabled when the HP definition includes it


27. Verify that cine playback plays frames in all viewports simultaneously when synced by HP


28. Verify that the Stack View toggle state in the UI matches the HP setting after HP application


29. Verify that disabling Stack View in the HP disables all cine controls and stops any playback



Negative & Edge
30. Verify that Stack View on a single-frame series does nothing and shows a warning if defined
31. Verify that rapidly toggling Stack View during playback does not freeze or corrupt the viewer state
32. Verify that playback speed settings revert to defaults if HP specifies unsupported speeds
33. Verify that Stack View can be applied independently per viewport if HP defines partial stack sync


---

‚úÖ Acceptance Criteria Mapping

Acceptance Criterion	Covered By Test Cases

AC1: Each global function must reflect HP-defined enable/disable state on load	1, 6, 13, 14, 21, 22, 28, 29
AC2: Synchronized operations must behave correctly when Sync is enabled	2, 3, 4
AC3: 3D Cursor must update across viewports when enabled	12
AC4: Cross Ref lines must track anatomy positions correctly	20
AC5: Stack View must allow simultaneous cine playback when enabled	27
AC6: Functions must be isolated (i.e., disabling one does not affect others)	6, 14, 22, 29
AC7: System handles invalid or unsupported function definitions gracefully	7, 15, 23, 24, 30
AC8: Toggling functions rapidly or mid-playback must not crash or desynchronize viewer	8, 18, 31
AC9: Function state persists correctly across HP re-applications	10, 17, 33



---

Let me know if you‚Äôd like these spun into a Gherkin feature file, manual test case tables, or automation scaffolds next!

