Absolutely, Apurva. Let‚Äôs bring your SRO-53 ‚Üí SSRS-305 pair to full QA excellence using your Automation Test Engineer persona.


---

üü© System Requirement: SEGV-SRO-53

> ‚ÄúThe system shall provide a set of default hanging protocols.‚Äù



‚úÖ Objective:

Ensure the system provides fallback display logic (via default HPs) when no learned/user-defined HP is matched.


---

üü® Sub-System Requirement: SEGV-SSRS-305

> ‚ÄúThe system shall display a 1x1 viewport grid on the primary monitor containing the first series from the primary study when no other layout definition is present.‚Äù



‚úÖ Objective:

If no HP is matched, system falls back to showing the first series from the primary study in a 1x1 layout on the primary monitor.


---

‚úÖ All Descriptive Test Scenarios for SRO-53 & SSRS-305

(Written using ‚ÄúVerify‚Ä¶‚Äù format, with Positive, Negative, and Edge scenarios‚Äîfully grounded in UI, backend, DICOM, and workflow behavior)


---

‚úÖ Positive Test Scenarios

1. Verify that the system applies the default 1x1 layout when no learned or user-defined hanging protocol is matched for a study


2. Verify that the first available series from the primary study is displayed in the 1x1 viewport when no layout is defined


3. Verify that the fallback 1x1 layout appears specifically on the primary monitor in single-monitor configurations


4. Verify that the system applies the fallback default layout consistently across modalities such as CT, MR, and US


5. Verify that the system applies the default layout even when the study contains multiple series without a matched HP


6. Verify that the default layout configuration is stored and retrieved correctly from system settings or configuration service


7. Verify that default HP is used when a new user with no preferences logs in and opens a study for the first time




---

‚ùå Negative Test Scenarios

8. Verify that the system displays a message or fallback content when the primary study contains no valid image series


9. Verify that the system does not crash when the default HP layout file is corrupted or malformed


10. Verify that an empty viewport is shown if the first series is not retrievable due to missing SOPInstanceUID or broken DICOM tags


11. Verify that the fallback layout still loads if the monitor configuration metadata is missing or incorrect


12. Verify that the viewer logs a warning or error if the default HP references a missing display group


13. Verify that no other protocols (manual, learned) override the fallback 1x1 layout once it's applied




---

‚ö†Ô∏è Edge Test Scenarios

14. Verify that when multiple series have the same Series Number, the viewer deterministically selects the first by acquisition time or SOPInstanceUID


15. Verify that in dual-monitor setups, the fallback 1x1 layout still renders on the primary monitor, not arbitrarily


16. Verify that the fallback 1x1 layout appears even when the study has partial metadata or non-standard compression (e.g., JPEG2000)


17. Verify that fallback 1x1 layout displays correctly on high-resolution or ultrawide monitors without stretching or misalignment


18. Verify that fallback applies instantly if the SRP engine times out or is disabled in configuration


19. Verify that fallback layout is not applied if a user-selected protocol exists even if SRP fails


20. Verify correct behavior when the system switches between online and offline mode (SRP service toggle)


21. Verify that the viewer reverts to fallback layout when the previously saved Learn Case was deleted before viewer launch




---

‚úÖ Relevant Acceptance Criteria (AC) Mapped to Test Scenarios

Acceptance Criteria	Covered By Test Cases

AC1: Default HP must apply when no other protocol is available	1, 2, 6, 7
AC2: Fallback layout must be 1x1	1, 3, 4, 5, 14
AC3: First series of primary study must be shown	2, 8, 10, 14
AC4: Must not crash or degrade in corrupted/missing config	9, 10, 11, 12
AC5: Must apply on primary monitor	3, 15
AC6: Must support edge devices and dual-screen scenarios	15, 17
AC7: System must distinguish between fallback vs matched protocol state	13, 19, 21



---

Would you like these transformed into:

Gherkin-style feature file

Manual test case table (with steps + expected)

Playwright automation code scaffold


Or should we move to the next SRO/SSRS pair?
Absolutely, Apurva! Here‚Äôs a clean separation of test cases:


---

üü© SRO-53: ‚ÄúThe system shall provide a set of default hanging protocols.‚Äù

‚úÖ End-to-End Test Scenarios for SRO-53

These validate the system-level behavior of fallback hanging protocols being provided and applied when no user-learned or predefined layout is available.

üî∑ Positive

1. Verify that the system applies the default 1x1 layout when no learned or user-defined hanging protocol is matched for a study


2. Verify that the default layout configuration is stored and retrieved correctly from system settings or configuration service


3. Verify that default HP is used when a new user with no preferences logs in and opens a study for the first time


4. Verify that the system applies the default layout consistently across different modalities such as CT, MR, and US



üî∂ Negative

5. Verify that the viewer does not crash when the default HP layout file is corrupted or malformed


6. Verify that the viewer logs a warning or error if the default HP references a missing display group


7. Verify that fallback layout is not applied if a user-selected protocol exists even if SRP fails



‚ö†Ô∏è Edge

8. Verify that fallback applies instantly if the SRP engine times out or is disabled in configuration


9. Verify correct behavior when the system switches between online and offline mode (SRP service toggle)


10. Verify that the viewer reverts to fallback layout when the previously saved Learn Case was deleted before viewer launch




---

üü® SSRS-305: ‚ÄúThe system shall display a 1x1 viewport grid on the primary monitor containing the first series from the primary study when no other layout definition is present.‚Äù

‚úÖ Functional Test Scenarios for SSRS-305

These focus on the actual fallback layout rendering behavior on the viewer side.

üî∑ Positive

1. Verify that the first available series from the primary study is displayed in the 1x1 viewport when no layout is defined


2. Verify that the 1x1 layout is displayed on the primary monitor in a single-monitor setup


3. Verify that the fallback layout is modality-agnostic and applies uniformly across supported modalities


4. Verify that the fallback 1x1 layout does not interfere with subsequent manual layout overrides by the user



üî∂ Negative

5. Verify that the system displays a message or fallback content when the primary study contains no valid image series


6. Verify that an empty viewport is shown if the first series is not retrievable due to missing SOPInstanceUID or broken DICOM tags


7. Verify that the fallback layout still loads if the monitor configuration metadata is missing or incorrect


8. Verify that no other protocols (manual, learned) override the fallback 1x1 layout once it is applied



‚ö†Ô∏è Edge

9. Verify that when multiple series have the same Series Number, the viewer deterministically selects the first by acquisition time or SOPInstanceUID


10. Verify that in dual-monitor setups, the fallback 1x1 layout still renders on the primary monitor, not arbitrarily


11. Verify that the fallback 1x1 layout displays correctly on high-resolution or ultrawide monitors without distortion


12. Verify that fallback 1x1 layout handles DICOM studies with non-standard compression (e.g., JPEG2000)




---

Would you like me to generate these in a structured table or export them into a specific format (Excel test suite, Gherkin, JSON)?
Or are we moving on to the next SRO/SSRS pair?

‚Äê---------------------
Thanks, Apurva. Let's continue with SRO-69 and its related SSRS-282, 306, and 307. I‚Äôve separated the test cases based on SRO and SSRS, and each scenario starts with "Verify" as you prefer.


---

‚úÖ SRO-69

Requirement: "The system shall allow user to save the current hanging state as a Learn Case."

üîπ End-to-End Test Scenarios for SRO-69

1. Verify that the system allows the user to trigger Learn Case saving from the viewer UI.


2. Verify that the system persists all applied hanging parameters in the saved Learn Case.


3. Verify that saving a Learn Case does not affect the ongoing study viewing session.


4. Verify that the system shows a success message upon successful Learn Case saving.


5. Verify that Learn Case saving fails gracefully if the backend service is down.


6. Verify that Learn Case saving is disabled for unsupported modalities.


7. Verify that Learn Case overwrites existing case if one already exists for the same user and context (if designed so).


8. Verify that Learn Case is saved with proper metadata: timestamp, user ID, study UID, etc.


9. Verify that saving is restricted when the user has read-only access.


10. Verify that system logs capture Learn Case save event with proper audit information.




---

‚úÖ SSRS-282

Requirement: "Front end application shall provide a Learn button to save the current hanging protocol."

üîπ Test Scenarios for SSRS-282

1. Verify that the Learn button is visible when a hanging protocol is applied.


2. Verify that clicking the Learn button initiates saving of current hanging parameters.


3. Verify that Learn button is disabled if there is no active hanging protocol.


4. Verify tooltip/help text is shown when hovering over the Learn button.


5. Verify button responsiveness across different resolutions and screen modes.


6. Verify that clicking Learn multiple times rapidly does not trigger duplicate saves.


7. Verify Learn button behavior for different roles: Radiologist vs Technician.


8. Verify UI feedback (e.g., spinner or success icon) after clicking Learn.


9. Verify that Learn button is not rendered on mobile mode or unsupported browsers.




---

‚úÖ SSRS-306

Requirement: "The system shall display a message indicating the SRP Learn Case could not be saved."

üîπ Test Scenarios for SSRS-306

1. Verify that an error message is displayed when Learn Case save fails due to server error.


2. Verify that message shows proper context like: ‚ÄúSave failed. Try again later.‚Äù


3. Verify that the error message disappears after a timeout or user dismissal.


4. Verify that no message is shown if save succeeds.


5. Verify that repeated save failures do not stack multiple messages.


6. Verify that localized error messages are shown in selected user language.


7. Verify that error message appears when the internet connection is lost during saving.


8. Verify that proper status code from API maps to meaningful user messages (e.g., 400, 500).




---

‚úÖ SSRS-307

Requirement: "The system shall wait for a response from the HP Service up to a configurable time threshold before applying a default hanging protocol."

üîπ Test Scenarios for SSRS-307

1. Verify that system waits for a Learn Case match response from HP service before applying HP.


2. Verify that default HP is applied when no response is received within the configured timeout.


3. Verify that the timeout is configurable from admin or config panel.


4. Verify system logs the timeout event for HP fallback usage.


5. Verify HP matching logic is skipped if default timeout is reached.


6. Verify that increasing the timeout value delays fallback to default HP.


7. Verify behavior when HP service sends partial or malformed response.


8. Verify fallback mechanism does not apply if timeout is set to 0.


9. Verify correct HP is applied if response is received just before timeout.




---

Let me know if you‚Äôd like the same format for another SRO next.

---------------72----------------
Absolutely, Apurva. Here's the full breakdown for:


---

üü© SRO-72

Requirement: "The system shall allow the user to select an alternate hanging protocol from a list of available hanging protocols."

> üîÅ This requirement focuses on enabling the user-driven selection of an HP (instead of automatic SRP selection) ‚Äî directly involving user interaction with UI (Learn dropdown) and confirming HP application.




---

‚úÖ End-to-End Test Scenarios for SRO-72

1. Verify that the system displays a list of available hanging protocols for user selection when the viewer is launched.


2. Verify that the user can open the list of hanging protocols through the UI dropdown.


3. Verify that the viewer updates its layout when a user selects a hanging protocol from the list.


4. Verify that only the selected hanging protocol is applied and no auto-selection overrides it.


5. Verify that the viewer maintains state consistency after switching from an auto-applied HP to a manually selected one.


6. Verify that the list of alternate HPs is generated based on Learn Cases relevant to the current study and context.


7. Verify that the applied protocol reflects in the audit trail with timestamp and user ID.


8. Verify that protocol selection remains stable during comparison addition/removal.


9. Verify that protocol switching is blocked or warned if done mid-way through a reading session (depending on business rules).


10. Verify that when no hanging protocols are available, the UI handles the empty state gracefully.




---

üü® SSRS-301

Requirement: ‚ÄúThe front-end application shall provide a drop-down menu from the HP Learn button with each menu item labeled with HP name and context matching score.‚Äù


---

üîπ Test Cases for SSRS-301

1. Verify that clicking the HP Learn button displays a dropdown with alternate hanging protocols.


2. Verify that each entry in the dropdown shows the hanging protocol name.


3. Verify that each hanging protocol is labeled with its context matching score.


4. Verify that the dropdown list is sorted by descending match score.


5. Verify that when no Learn Cases are available, the dropdown is either empty or hidden.


6. Verify tooltip, hover state, or extended info is shown for long-named protocols.


7. Verify that dropdown rendering adapts properly on small or ultra-wide screen sizes.


8. Verify that the dropdown can be navigated via keyboard (up/down/enter).


9. Verify that the dropdown closes when clicking outside the viewer frame or menu.


10. Verify that the matching score shown is accurate and comes from the SRP engine logic.




---

üü® SSRS-302

Requirement: ‚ÄúUpon user selection of a specific menu item from the HP Learn button drop-down, the front-end application shall apply the selected Hanging Protocol.‚Äù


---

üîπ Test Cases for SSRS-302

1. Verify that selecting a hanging protocol from the dropdown applies it immediately in the viewer.


2. Verify that the selected protocol's layout, viewport mapping, and rendering parameters are reflected in the viewer state.


3. Verify that applying a new HP replaces the previously applied HP completely.


4. Verify that a loading spinner or message is shown while the selected HP is being applied.


5. Verify that an error message is shown when applying a corrupted or unavailable HP.


6. Verify that user selection of HP is recorded in logs or audit trail with user ID and timestamp.


7. Verify that if the user re-selects the same HP, no redundant re-application occurs.


8. Verify that the system can gracefully apply HP even if some referenced series are missing.


9. Verify that each component of the selected HP (layout, sync tools, cross-ref) is respected in the UI after application.


10. Verify fallback behavior when HP application partially fails (e.g., missing one series in layout).




---

‚úÖ Acceptance Criteria Mapping (AC)

AC Description	Covered In Test Cases

User should be able to see all matching alternate HPs	SRO-72 TC 1, SSRS-301 TC 1, 2, 4
User should see match score for context relevance	SSRS-301 TC 3, 10
User must be able to select and apply a hanging protocol	SRO-72 TC 3, SSRS-302 TC 1, 2
System must apply selected HP and update viewer layout	SSRS-302 TC 2, 3, 9
Audit trail must reflect user protocol selection	SRO-72 TC 7, SSRS-302 TC 6
Viewer must handle errors and unavailable/malformed HPs	SSRS-302 TC 5, 10
Dropdown should be fully functional (keyboard, responsive, accessible)	SSRS-301 TC 7, 8, 9



---

Would you like the same level of detail for SRO-73 or 75 next, or convert these into a tabular format or Gherkin?

-------------------73----------------

Certainly, Apurva! Here's the detailed breakdown for:


---

üü© SRO-73

Requirement:
‚ÄúThe system shall support the selection of comparison studies as part of hanging protocol application.‚Äù


---

‚úÖ End-to-End Test Scenarios for SRO-73

1. Verify that the system allows the user to select one or more comparison studies during hanging protocol application.


2. Verify that the selected comparison studies are included in the context used for hanging protocol matching.


3. Verify that the viewer correctly displays both current and selected comparison studies after HP is applied.


4. Verify that Learn Case selection and scoring include the selected comparison studies in context matching.


5. Verify that removing a comparison study re-evaluates the hanging protocol if supported by the system design.


6. Verify that HPs with embedded comparison logic behave correctly when comparisons are added manually.


7. Verify that system audit logs include which comparison studies were selected when applying the HP.


8. Verify fallback behavior when comparison studies are not available or cannot be loaded.




---

üü® SSRS-303

Requirement:
‚ÄúThe front-end application shall include comparison studies as part of the study context in the hanging protocol.‚Äù


---

üîπ Test Cases for SSRS-303

‚úÖ Positive Scenarios

1. Verify that the viewer includes selected comparison studies as part of the hanging protocol matching context.


2. Verify that comparison studies are displayed in the designated comparison viewports as per the hanging protocol definition.


3. Verify that the HP engine re-evaluates and ranks Learn Cases based on the comparison study‚Äôs context match.


4. Verify that when two Learn Cases differ only by comparison study presence, the better match is selected.


5. Verify that multiple comparison studies can be selected and included in the Learn Case context.



‚ùå Negative Scenarios

6. Verify that an error is displayed if a selected comparison study is corrupt or fails to load.


7. Verify that the system skips unusable comparison studies during protocol application.


8. Verify that mismatched modality comparison studies do not break the layout or matching logic.



‚ö†Ô∏è Edge Scenarios

9. Verify that if comparison studies are added after HP is applied, the viewer doesn‚Äôt reapply HP unless triggered.


10. Verify that selecting a comparison study with only non-diagnostic images does not interfere with HP layout.


11. Verify that the HP service still functions when a mix of DICOM-compliant and non-compliant comparison studies is used.


12. Verify that comparison study metadata (e.g., StudyDate, Modality, StudyUID) is passed correctly in the API call for HP selection.




---

‚úÖ Acceptance Criteria Mapping

Acceptance Criteria	Covered Test Cases

HP context should include selected comparison studies	SRO-73 TC 2, SSRS-303 TC 1, 3, 12
Viewer should render comparison studies alongside current study	SRO-73 TC 3, SSRS-303 TC 2
Matching should prioritize Learn Cases with better comparison context match	SRO-73 TC 4, SSRS-303 TC 4
Comparison studies should not cause errors or break layout	SRO-73 TC 8, SSRS-303 TC 6, 7, 8
System should handle multi-comparison gracefully	SSRS-303 TC 5, 9, 10, 11
Proper logging and metadata passing required	SRO-73 TC 7, SSRS-303 TC 12



---

Let me know if you'd like this batch exported in test case table format, or if you‚Äôre ready to proceed to SRO-75 or another next pair.


Absolutely, Apurva. You‚Äôve got it ‚Äî I‚Äôll now dissect SSRS-289 down to the finest implementation detail, just like a 10+ year automation test engineer would for a radiology imaging system like GenesisViewer.


---

üü® SSRS-289 (Detailed Breakdown)

Requirement: "The front-end application shall apply the display/rendering parameters as per the hanging protocol for each viewport."
Applies To: GenesisViewer (HP Client)
Parent SRO: SRO-74


---

üîç Key Functional Components to Cover in Testing:

Rendering Parameter	DICOM Tag(s) / Source	Notes

WW/WL	(0028,1050) / (0028,1051)	Must apply per viewport per series
Zoom & Pan	Internal viewer state (usually float/scalar values)	May vary by modality or protocol
Orientation	(0020,0037) ‚Äì Image Orientation Patient	Critical for anatomical alignment
Shutters	(0018,1622)‚Äì(0018,1624) or Presentation State	Optional ‚Äì should not break viewer
VOI LUT / Presentation State	(0028,3010) / (0028,3110) etc.	DICOM-defined grayscale transforms
Per-viewport mapping	Series UID or display group mapping	Must not cross-apply settings



---

‚úÖ Test Cases ‚Äì Organized by Parameter & Scenario

Each starts with "Verify" and is grouped by what rendering logic is being tested.


---

üß™ A. WW/WL (Window Width / Window Level)

1. Verify that the WW/WL values defined in the hanging protocol are applied to each viewport upon layout initialization.


2. Verify that different WW/WL values are applied per viewport when different series are loaded.


3. Verify that default WW/WL presets are used when HP does not define values.


4. Verify that WW/WL values are overridden only when the user explicitly changes them.


5. Verify that reapplying the same HP resets WW/WL to original values.


6. Verify that WW/WL values persist through study reload when HP is reapplied.




---

üß™ B. Zoom & Pan

7. Verify that zoom values defined in the HP are correctly applied to each viewport.


8. Verify that pan (x/y offset) is correctly restored per viewport from the HP.


9. Verify that zoom/pan settings are not applied globally unless the HP defines global sync.


10. Verify correct scaling/centering behavior on high-DPI displays or ultrawide screens.


11. Verify zoom behavior is modality-aware (e.g., disable for ultrasound if defined so).


12. Verify fallback to default zoom when HP zoom parameters are invalid or missing.




---

üß™ C. Orientation

13. Verify that image orientation (e.g., HFS, HFP) is applied as defined in the HP per viewport.


14. Verify that each viewport respects its assigned orientation regardless of other viewports.


15. Verify that flipping or rotating images based on orientation settings reflects in overlay.


16. Verify viewer gracefully handles missing or malformed orientation tags.


17. Verify that user override does not persist after HP is re-applied.




---

üß™ D. Shutters & VOI LUTs / Presentation State

18. Verify that shutters are rendered in viewports when specified in the HP.


19. Verify that VOI LUTs are applied per series if provided in the HP.


20. Verify that missing or malformed LUTs result in safe fallback (grayscale default).


21. Verify that overlapping shutters from DICOM Presentation State are ignored if HP disables them.


22. Verify correct rendering when Presentation State is present but HP defines custom WW/WL.




---

üß™ E. Per-Viewport Isolation

23. Verify that rendering settings in one viewport do not affect another.


24. Verify that switching series in one viewport maintains its unique render settings.


25. Verify that layout reset applies only the HP settings, not cached user overrides.




---

üß™ F. General/Meta Tests

26. Verify that rendering parameters are applied in the correct order: layout ‚Üí mapping ‚Üí rendering.


27. Verify that hanging protocol reapplication completely resets the viewport state.


28. Verify that audit logs reflect which rendering parameters were applied per viewport.


29. Verify that mismatched series types (e.g., one MPR and one 2D axial) still receive correct rendering.


30. Verify consistent rendering behavior when switching between Learn Case and default HP.




---

‚ö†Ô∏è Edge Case Test Scenarios

31. Verify that rendering applies correctly to multi-frame DICOM series.


32. Verify rendering in stacked view mode where the same series spans multiple viewports.


33. Verify correct behavior when images are compressed (e.g., JPEG2000, RLE).


34. Verify behavior when SOPInstanceUID referenced in HP is not present in current study.


35. Verify that rendering logic gracefully handles series of vastly different dimensions or matrix sizes.




---

‚úÖ Acceptance Criteria (Mapped)

Acceptance Criteria	Covered Test Cases

Rendering parameters from HP must be applied to all viewports	1‚Äì6, 7‚Äì12, 13‚Äì17
Each viewport must have isolated rendering behavior	23‚Äì25
HP logic must handle missing/invalid rendering data gracefully	3, 6, 12, 16, 20, 34
VOI LUTs, shutters, and orientation must not cause viewer instability	18‚Äì22, 29, 33
Auditability and logging must trace rendering application	28
Rendering order and layout interaction must be correct	26, 27
Compatibility with compressed, multi-frame, and large image data	31‚Äì35



---

Would you like these converted into:

Structured test case document (Excel/test suite)?

Gherkin for automation?

Or shall we proceed with SSRS-290 in this same deep format?


Let‚Äôs keep the momentum going.

---------------------290---------------
Here‚Äôs the minute-level breakdown and full set of ‚ÄúVerify‚Ä¶‚Äù test scenarios for SSRS-290, organized by each global function. I‚Äôve also included the relevant acceptance criteria mapping at the end.


---

üü® SSRS-290

Requirement:
‚ÄúThe front-end application shall enable/disable the global application functions like Sync, 3D Cursor, Cross Ref, Stack View for each viewport based on the hanging protocol.‚Äù

Parent SRO: SRO-74
Impact: HP Client (GenesisViewer front-end)


---

üîç Key Functions & Implementation Notes

Function	Description	UI Element / State Source

Sync	Synchronizes scrolling/zooming/panning across viewports	Toggle button; internal sync flag
3D Cursor	Shows/hides 3D cursor crosshair and ROI tracking across series	Cursor icon; 3D cursor state
Cross Ref	Cross-reference lines between viewports to correlate anatomy	Overlay lines; cross-ref flag
Stack View	Enables cine/stack playback mode for multi-frame series	Stack/play button; stack mode state



---

‚úÖ Detailed Test Scenarios

A. Sync

1. Verify that Sync is enabled in all viewports when the hanging protocol defines synchronization


2. Verify that all synchronized viewports scroll together when the user scrolls one viewport


3. Verify that zooming in one viewport zooms all synced viewports by the same factor


4. Verify that pan operations in one viewport pan all synced viewports equally


5. Verify that Sync toggle in the toolbar reflects the HP setting (on/off) upon HP application


6. Verify that disabling Sync in the HP results in independent scrolling/zoom behavior per viewport



Negative & Edge
7. Verify that Sync does not activate if there is only one viewport
8. Verify that Sync toggling while cine play is active does not crash the viewer
9. Verify that partial sync (e.g., only zoom synced) is applied if the HP specifies sub-feature granularity
10. Verify that sync state persists through layout re-applications but resets when a different HP is chosen


---

B. 3D Cursor

11. Verify that the 3D Cursor is visible in viewports when enabled by the hanging protocol


12. Verify that moving the cursor in one viewport updates the cursor position in all linked 3D viewports


13. Verify that the 3D Cursor toggle reflects the HP definition immediately after HP is applied


14. Verify that disabling the 3D Cursor in the HP hides the cursor overlay in all viewports



Negative & Edge
15. Verify that 3D Cursor operations do not throw exceptions when applied to single-slice series
16. Verify that the cursor update logic handles missing slice positions gracefully (e.g., gaps in Z-axis)
17. Verify that enabling 3D Cursor on a non-volume series (e.g., CR) has no effect and shows no errors
18. Verify that rapid toggling of 3D Cursor does not lead to state desynchronization between viewports


---

C. Cross Ref

19. Verify that Cross-Reference lines appear across viewports when enabled by the hanging protocol


20. Verify that moving the reference line in one viewport updates the corresponding lines in all others


21. Verify that the Cross Ref toggle state matches the HP definition upon HP load


22. Verify that disabling Cross Ref removes all overlay lines immediately



Negative & Edge
23. Verify that Cross Ref does not apply on modalities that lack spatial correlation (e.g., US)
24. Verify that malformed line coordinates in the HP do not crash the viewer but fall back silently
25. Verify that Cross Ref functionality survives viewport reordering or layout changes mid-session


---

D. Stack View (Cine Mode)

26. Verify that Stack View (cine playback controls) is enabled when the HP definition includes it


27. Verify that cine playback plays frames in all viewports simultaneously when synced by HP


28. Verify that the Stack View toggle state in the UI matches the HP setting after HP application


29. Verify that disabling Stack View in the HP disables all cine controls and stops any playback



Negative & Edge
30. Verify that Stack View on a single-frame series does nothing and shows a warning if defined
31. Verify that rapidly toggling Stack View during playback does not freeze or corrupt the viewer state
32. Verify that playback speed settings revert to defaults if HP specifies unsupported speeds
33. Verify that Stack View can be applied independently per viewport if HP defines partial stack sync


---

‚úÖ Acceptance Criteria Mapping

Acceptance Criterion	Covered By Test Cases

AC1: Each global function must reflect HP-defined enable/disable state on load	1, 6, 13, 14, 21, 22, 28, 29
AC2: Synchronized operations must behave correctly when Sync is enabled	2, 3, 4
AC3: 3D Cursor must update across viewports when enabled	12
AC4: Cross Ref lines must track anatomy positions correctly	20
AC5: Stack View must allow simultaneous cine playback when enabled	27
AC6: Functions must be isolated (i.e., disabling one does not affect others)	6, 14, 22, 29
AC7: System handles invalid or unsupported function definitions gracefully	7, 15, 23, 24, 30
AC8: Toggling functions rapidly or mid-playback must not crash or desynchronize viewer	8, 18, 31
AC9: Function state persists correctly across HP re-applications	10, 17, 33



---

Let me know if you‚Äôd like these spun into a Gherkin feature file, manual test case tables, or automation scaffolds next!

-----------------76------
Here‚Äôs the deep-dive breakdown for SRO-76 and its two child SSRS, with every scenario starting with ‚ÄúVerify‚Äù, separated by SRO vs. SSRS, and ending with an AC-to-test mapping.


---

üü© SRO-76

Requirement:

> ‚ÄúThe system shall auto-apply a single hanging protocol when the viewer is launched for a given study.‚Äù



üîπ End-to-End Test Cases for SRO-76

1. Verify that on viewer launch, the SRP engine is queried for the best-matched hanging protocol.


2. Verify that the hanging protocol with the highest confidence score is the one applied.


3. Verify that only one hanging protocol is applied‚Äîeven if multiple protocols exceed the confidence threshold.


4. Verify that manual user overrides after launch do not re-trigger auto-application.


5. Verify that if the SRP engine returns no matches, the system falls back to the default hanging protocol.


6. Verify that the applied hanging protocol‚Äôs layout and rendering exactly match the Learn Case definition.


7. Verify that audit logs record which hanging protocol was auto-applied, including its ID and confidence score.




---

üü® SSRS-288

Requirement:

> ‚ÄúThe front-end application shall apply the viewport layout and image-display-group mapping as defined in the selected hanging protocol.‚Äù



üîç Breakdown of Rendering Logic

Aspect	Source in HP	Notes

Grid Definition (rows √ó cols)	HP.layout.grid	e.g., ‚Äú2x2‚Äù, ‚Äú3x3‚Äù
Monitor Mapping	HP.layout.monitorIds	Primary vs. secondary monitor targets
Display-Group ‚Üí Viewport Mapping	HP.mapping.groups	e.g., ‚Äúaxial ‚Üí viewport1‚Äù
Series Selection Order	HP.mapping.seriesKey	e.g., by SeriesNumber or AcquisitionTime
Multi-Monitor Splits	HP.layout.splits	e.g., 2 viewports on each monitor


üîπ Test Scenarios for SSRS-288

‚úÖ Positive

1. Verify that the grid definition from the HP (e.g., 2√ó2) is rendered correctly in the viewer.


2. Verify that each image display group is mapped to its designated viewport.


3. Verify that multi-monitor layouts split viewports across monitors as defined.


4. Verify that series selection for each display group uses the specified DICOM tag (SeriesNumber).


5. Verify that if two display groups map to the same viewport, the viewer throws a layout conflict warning.



‚ùå Negative

6. Verify that a malformed grid definition (e.g., ‚Äú2√ó‚Äù) results in fallback to a safe default (1√ó1).


7. Verify that missing display-group entries leave the affected viewport blank but do not crash.


8. Verify that invalid monitor IDs in the HP cause all viewports to render on the primary monitor.



‚ö†Ô∏è Edge

9. Verify that extra display-group entries beyond grid capacity are ignored with a logged warning.


10. Verify that swapping monitor order (primary/secondary reversed) still honors the HP‚Äôs monitorIds.


11. Verify that the viewport mapping works when the same display group appears multiple times.


12. Verify that layout is reapplied correctly if the HP is switched mid-session.




---

üü® SSRS-307

Requirement:

> ‚ÄúThe front-end application shall wait for a response from the HP Service up to a configurable time threshold before applying a default hanging protocol.‚Äù



üîç Breakdown of Timeout Logic

Aspect	Configuration	Notes

Timeout Threshold	config.hpServiceTimeoutMs	e.g., 3000 ms
Fallback Protocol	config.defaultHangingProtocol	ID of default HP to apply after TO
Response Handling	HPService API callback	success vs. failure vs. late
Logging	HPServiceTimeoutEvent	logs time, study UID, action


üîπ Test Scenarios for SSRS-307

‚úÖ Positive

1. Verify that the system waits up to the configured timeout before applying any protocol.


2. Verify that if the HP Service responds in-time, its protocol is applied without fallback.


3. Verify that raising the timeout value delays fallback accordingly.



‚ùå Negative

4. Verify that if no response arrives within the timeout, the default hanging protocol is applied.


5. Verify that a failed or malformed response (HTTP 500 or invalid JSON) triggers fallback.


6. Verify that multiple sequential timeouts do not crash the viewer but always apply default HP.



‚ö†Ô∏è Edge

7. Verify that setting timeout=0 results in immediate fallback without waiting.


8. Verify that a response arriving just after timeout is ignored and does not override the fallback.


9. Verify that rapid toggling of this timeout config (e.g., via admin panel) is picked up on next launch.


10. Verify that a late valid response is logged but does not disrupt the already-applied default layout.




---

‚úÖ Acceptance Criteria Mapping

Acceptance Criterion	Covered By

AC1: System auto-applies only one HP on viewer launch	SRO-76 TC 1, 2, 3
AC2: Applies the HP with highest confidence score	SRO-76 TC 2
AC3: Falls back to default HP if service fails or times out	SRO-76 TC 5; SSRS-307 TC 4
AC4: Respects the configurable timeout threshold	SSRS-307 TC 1, 3
AC5: Layout & mapping must match HP definition exactly	SRO-76 TC 6; SSRS-288 TC 1-5, 9
AC6: HP Service timeout events are logged for audit	SRO-76 TC 7; SSRS-307 TC 10
AC7: Invalid HP definitions cause safe fallback without crash	SSRS-288 TC 6, 7; SSRS-307 TC 5



---

Let me know if you‚Äôd like these in Gherkin, a spreadsheet-style matrix, or as automation skeletons next!

-----------------------------76-----------------------
Below is the breakdown for SRO-76 (parent) and its SSRS-288 (rendering parameters) with every single minute detail covered as you requested.  Each scenario starts with ‚ÄúVerify‚Ä¶‚Äù and they‚Äôre clearly separated by SRO vs SSRS.


---

üü© SRO-76

> ‚ÄúThe system shall auto-apply a single hanging protocol when the viewer is launched for a given study.‚Äù



üîπ End-to-End Test Cases for SRO-76 (Rendering-focused)

1. Verify that on viewer launch the SRP engine is queried and returns a single HP including its rendering parameters.


2. Verify that the returned HP‚Äôs rendering parameters (viewType, WW/WL, RenderingMode, etc.) arrive in the client callback.


3. Verify that those rendering parameters are applied exactly to each viewport without user intervention.


4. Verify that if the HP service fails or times out, the default HP (with its own rendering params) is applied.


5. Verify that manual user changes to rendering after launch do not overwrite the auto-applied HP unless re-triggered.




---

üü® SSRS-288

> ‚ÄúThe front-end application shall apply the display/rendering parameters as per the hanging protocol for each viewport.‚Äù



Rendering Parameters List (from your image):

viewType

WindowWidth

WindowLevel

RenderingMode

ThdRenderingMode

Rotate/Flip/Zoom (Supported only for Native viewType)



---

A. viewType

1. Verify that the viewType defined in the HP (e.g. ‚ÄúAxial‚ÄìMIP‚Äù) is applied to the viewport on load.


2. Verify that all supported viewTypes (Native, Axial(Standard, MIP, MinIP, Average, VR), Sagittal, Coronal, 3D) render correctly.


3. Verify that an unknown viewType in the HP falls back to ‚ÄúNative‚Äù without error.


4. Verify that switching the HP mid-session updates each viewport‚Äôs viewType immediately.




---

B. WindowWidth & WindowLevel

5. Verify that WindowWidth and WindowLevel values from the HP are set on each viewport‚Äôs LUT.


6. Verify that invalid WW/WL values (e.g. negative, zero) in the HP revert to the viewer‚Äôs default WW/WL.


7. Verify that extreme WW/WL values (e.g. very narrow or very wide) are clamped to supported ranges.


8. Verify that modifying WW/WL manually after HP application does not persist across HP re-application.




---

C. RenderingMode

9. Verify that the RenderingMode parameter (e.g. ‚ÄúStandard‚Äù, ‚ÄúMIP‚Äù, ‚ÄúMinIP‚Äù, ‚ÄúVR‚Äù) is honored per viewport.


10. Verify that unsupported RenderingMode values in the HP default to ‚ÄúStandard‚Äù without crashing.


11. Verify that RenderingMode changes are synchronized if Sync is enabled (per SSRS-290).




---

D. ThdRenderingMode

12. Verify that the ThdRenderingMode (threshold rendering, e.g. bone-only) from the HP is applied if defined.


13. Verify that missing threshold settings fall back to the HP‚Äôs RenderingMode.


14. Verify that out-of-range threshold values (e.g. >255) are clamped or ignored gracefully.




---

E. Rotate / Flip / Zoom (Native Views Only)

15. Verify that Rotate, Flip, and Zoom operations defined in the HP are executed only when viewType=Native.


16. Verify that Rotate/Flip commands are ignored if viewType‚â†Native, with no console errors.


17. Verify that Zoom levels from the HP are applied exactly, and invalid zoom factors (e.g. negative) revert to 100%.


18. Verify that multiple sequential rotate/flip operations apply in the correct order (Rotate ‚Üí Flip).




---

‚úÖ Acceptance Criteria Mapping

AC #	Description	Covered by Test Cases

AC1	HP rendering parameters must be applied to each viewport on launch	A1, A2, B5, C9, D15
AC2	Unsupported or invalid parameter values fall back to safe defaults	A3, B6, C10, D16
AC3	viewType, WW/WL, RenderingMode, ThdRenderingMode, Rotate/Flip/Zoom behave exactly per HP spec	All above
AC4	Parameters re-apply correctly if HP is switched mid-session	A4, B8, C11, D18
AC5	Manual adjustments do not persist across HP re-application	B8, D18



---

Next Steps?

I can lay these out in a spreadsheet or Gherkin feature file.

Or we can jump to SSRS-307 or the next SRO you have. Let me know!

--------------------------307--------------------------

Below is the minute-detail breakdown for SSRS-307, with every test scenario starting with ‚ÄúVerify‚Ä¶‚Äù, organized by functional aspect, followed by Acceptance Criteria mapping.


---

üü® SSRS-307

Requirement:

> ‚ÄúThe system shall wait for a response from the HP Service up to a configurable time threshold before applying a default hanging protocol.‚Äù



Parent SRO: SRO-76
Impact: HP Client (GenesisViewer front-end)


---

üîç Key Functional Aspects

Aspect	Description

Timeout Threshold	Configurable interval (ms or s) the client will wait for HP Service response
Service Response	Success payload (HP definition) or failure (error, malformed data, no response)
Fallback Protocol	Default hanging protocol applied after timeout or on service failure
Logging & Metrics	Client-side audit of wait duration, fallback event, and service response
Reactivity	How the viewer UI shows waiting state (spinner), then switches to default HP on timeout



---

‚úÖ Detailed Test Scenarios

A. Configurable Timeout Behavior

1. Verify that the client reads the HP service timeout value from configuration on startup.


2. Verify that increasing the timeout value (e.g., from 3s to 10s) delays fallback proportionally.


3. Verify that setting timeout to zero results in immediate fallback without invoking service call.


4. Verify that invalid timeout values (negative, non-numeric) revert to a safe default (e.g., 3s).


5. Verify that changes to timeout in an admin panel take effect on the next viewer launch.



B. Successful Service Response Before Timeout

6. Verify that if the HP Service returns a valid HP definition within the timeout, that HP is applied.


7. Verify that the client stops the timeout countdown immediately on receiving a valid response.


8. Verify that no fallback HP is applied if a valid response arrives even 1ms before timeout.


9. Verify that the UI removes any loading spinner once a response is processed.



C. Service Failure or No Response (Timeout Fallback)

10. Verify that if the HP Service does not respond within the timeout, the default HP is applied automatically.


11. Verify that a request error (HTTP 500) triggers immediate fallback, ignoring remaining time.


12. Verify that a malformed or partially invalid response (invalid JSON) triggers fallback.


13. Verify that network disconnects or timeouts at the HTTP layer still result in fallback.


14. Verify that the fallback HP used matches the configured default (e.g., 1√ó1 grid).



D. Logging & Audit

15. Verify that the client logs the actual wait duration and timestamp when fallback occurs.


16. Verify that the log entry includes the reason (timeout vs. error vs. malformed response).


17. Verify that successful service responses are logged with response time metrics.


18. Verify that logs do not contain sensitive patient data‚Äîonly protocol IDs and timestamps.



E. UI Feedback & User Experience

19. Verify that a visual loading indicator (spinner or status message) is shown while waiting.


20. Verify that the loading indicator disappears and default HP renders immediately after timeout.


21. Verify that switching away from the study before timeout cancels the wait and no fallback occurs.


22. Verify that multiple rapid launches (open/close) do not queue multiple timeouts or fallbacks.




---

‚ö†Ô∏è Edge Test Scenarios

23. Verify behavior when service responds precisely at the timeout boundary (race condition).


24. Verify that very large timeout values (e.g., 1 hour) do not block UI permanently.


25. Verify that mid-session HP re-applications also respect the timeout logic.


26. Verify that fallback logic works correctly in offline mode (no network).


27. Verify that partial responses (e.g., missing required HP fields) are treated as failures.


28. Verify that toggling timeout config at runtime (if supported) does not break current wait.




---

‚úÖ Acceptance Criteria Mapping

AC #	Description	Covered By

AC1	Client waits up to the configured timeout before fallback	A1‚ÄìA4, A5
AC2	Valid service response within timeout must apply HP, not fallback	B6‚ÄìB9
AC3	Fallback HP applied on service error or no response	C10‚ÄìC14
AC4	Logging of wait duration, reason, and HP IDs without patient data	D15‚ÄìD18
AC5	UI feedback visible during wait and removed on response or fallback	E19‚ÄìE22
AC6	Edge conditions (race at boundary, offline mode) handled gracefully	23‚Äì28



---

üëç Ready to convert these into Gherkin or automation code, or shall we tackle the next requirement?


